{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# postagger.tag(tokens=['How','are','you','?'])\n",
    "\n",
    "# ner_tagger.tag('Rami Eid is studying at Stony Brook University in NY'.split()) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nlp = \n",
    "\n",
    "# def relationship_Identifier(sentence):\n",
    "# #     nlp = StanfordCoreNLP(r'stanford-corenlp-full-2018-10-05')\n",
    "#     output = nlp.annotate(sentence, properties={\"annotators\":\"tokenize,ssplit,pos\",\n",
    "#                                      \"outputFormat\": \"json\",\"openie.triple.strict\":\"true\",\"splitter.disable\" : \"true\"})\n",
    "\n",
    "#     if (type(output) is str):\n",
    "#         output = json.loads(output, strict=False)\n",
    "#     #pickle.dump(output, open( \"save.p\", \"wb\" ))\n",
    "\n",
    "#     result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "#     #print(result)\n",
    "#     for i in result:\n",
    "#         for rel in i:\n",
    "#             relationSent=rel['relation'],rel['subject'],rel['object']\n",
    "#             print(relationSent)\n",
    "\n",
    "# sent = nltk.sent_tokenize('I hope you are doing good')\n",
    "# for s in sent:\n",
    "#     s = str(s)\n",
    "#     relationship_Identifier(s)\n",
    "\n",
    "# output = core_nlp.annotate('How are you? What are you doing?', properties=core_nlp_pos_props)\n",
    "\n",
    "# output[\"sentences\"][0]['tokens'][0]\n",
    "\n",
    "# len()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# text = (\n",
    "#   'Pusheen and Smitha walked along the beach. '\n",
    "#   'Pusheen wanted to surf, but fell off the surfboard.')\n",
    "\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# output = nlp.annotate(text, properties={\n",
    "#   'annotators': 'tokenize,ssplit,pos,depparse,parse',\n",
    "#   'outputFormat': 'json'\n",
    "#   })\n",
    "\n",
    "# output = nlp.semgrex(text, pattern='{tag: VBD}', filter=False)\n",
    "# print(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# blob = TextBlob(df.pronoun_resolved_text[0])\n",
    "# print(blob.noun_phrases)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def getCharacters(story_text):\n",
    "# #     text = nltk.pos_tag(nltk.word_tokenize(story_text))\n",
    "# #     ch = [word.lower() for word,pos in text if pos == 'NNP']\n",
    "# #     noun_set = set([word.lower() for word,pos in text if pos == 'NN'])\n",
    "# #     for word in noun_set:\n",
    "# #         if(isLiving(word)):\n",
    "# #             ch.append(word)\n",
    "# #     ch = set(ch)\n",
    "# #     if('mr' in ch):\n",
    "# #         ch.remove('mr')\n",
    "# #     if('mrs.' in ch):\n",
    "# #         ch.remove('mrs.')\n",
    "# #     return ch\n",
    "\n",
    "# # def getCharacters(story_text):\n",
    "# #     ch = findPersons(story_text)\n",
    "# #     output = core_nlp.annotate(story_text, properties=core_nlp_pos_props)\n",
    "# #     for sent in output['sentences']:\n",
    "# #         for words in sent['tokens']:\n",
    "# #             pos_tag = words['pos']\n",
    "# #             word = words['word']\n",
    "# #             if(pos_tag == 'NN'):\n",
    "# #                 if(isLiving(word)):\n",
    "# #                     ch.append(word.lower())\n",
    "# #     return set(ch)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
