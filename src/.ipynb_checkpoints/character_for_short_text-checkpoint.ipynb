{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = %time spacy.load('en_coref_lg')\n",
    "core_nlp = StanfordCoreNLP('http://localhost:9002')\n",
    "core_nlp_pos_props = {\"annotators\":\"tokenize,ssplit,pos\",\"outputFormat\": \"json\"}\n",
    "core_nlp_ie_props = {\"annotators\":\"openie\",\\\n",
    "                     \"outputFormat\": \"json\",\"openie.triple.strict\":\"true\",\"splitter.disable\" : \"true\"}\n",
    "path_data_index = '../data/bedtime_stories_data/index.csv'\n",
    "ner_tagger = StanfordNERTagger('../resources/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\\\n",
    "                               '../resources/stanford-ner-2018-10-16/stanford-ner.jar',\\\n",
    "                              encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findPersons(story_text):\n",
    "    ret = set()\n",
    "    for sent in nltk.sent_tokenize(story_text):\n",
    "        tokens = nltk.tokenize.word_tokenize(sent)\n",
    "        tags = ner_tagger.tag(tokens)\n",
    "        for tag in tags:\n",
    "            if (tag[1]=='PERSON'): \n",
    "                ret.add(tag[0].lower())\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCharacters(story_text):\n",
    "    ch = findPersons(story_text)\n",
    "    output = core_nlp.annotate(story_text, properties=core_nlp_pos_props)\n",
    "    for sent in output['sentences']:\n",
    "        for words in sent['tokens']:\n",
    "            pos_tag = words['pos']\n",
    "            word = spell(words['word'])\n",
    "            if(pos_tag == 'NN'):\n",
    "                if(isLiving(word)):\n",
    "                    ch.add(word.lower())\n",
    "    return ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_story_text(path_story):\n",
    "    file = open(path_story, 'r')\n",
    "    text = file.read()\n",
    "    text = text.replace('\\n',' ')\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isLiving(word):\n",
    "    wn_word = word + '.n.01'\n",
    "    try:\n",
    "        wn_lemma = wn.synset(wn_word)\n",
    "    except:\n",
    "        return False\n",
    "    l_hyper = str(list(wn_lemma.closure(lambda s: s.hypernyms())))\n",
    "    if('person.n.01' in l_hyper or 'animal.n.01' in l_hyper):\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resolvePronouns(story_text):\n",
    "    doc = nlp(story_text)\n",
    "    return doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractRelationships(story_text):\n",
    "    ret = []\n",
    "    sentences = nltk.sent_tokenize(story_text)\n",
    "    for sent in sentences:\n",
    "        output = core_nlp.annotate(sent,properties=core_nlp_ie_props)\n",
    "        result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "        for element in result:\n",
    "            for relation in element:\n",
    "                ret.append((relation['relation'],relation['subject'],relation['object']))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findMoreCharacters(old_char_set,phrase):\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    if(len(words) == 1):\n",
    "        if(isLiving(words[0])):\n",
    "            old_char_set.add(words[0].lower())\n",
    "    return old_char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateCharacters(old_char_set, relations):\n",
    "    for rel in relations:\n",
    "        old_char_set = findMoreCharacters(old_char_set,rel[1])\n",
    "        old_char_set = findMoreCharacters(old_char_set,rel[2])\n",
    "    return old_char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_data_index)\n",
    "df['story_text'] = df.path.apply(return_story_text)\n",
    "df['characters'] = df.story_text.apply(getCharacters)\n",
    "df['pronouns_resolved_text'] = df.story_text.apply(resolvePronouns)\n",
    "df['relations'] = df.pronouns_resolved_text.apply(extractRelationships)\n",
    "df['characters'] = df.apply(lambda row: updateCharacters(row['characters'], row['relations']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
